{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7699895,"sourceType":"datasetVersion","datasetId":4494557},{"sourceId":9375474,"sourceType":"datasetVersion","datasetId":5686839},{"sourceId":9422317,"sourceType":"datasetVersion","datasetId":5723023}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n'''import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))'''\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport sklearn.metrics\nfrom tqdm import tqdm\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport os\nfrom sklearn.decomposition import PCA\n\n# Dataset class for loading video deepfake detection data\nclass NumpyVideoDeepfakeDataset(Dataset):\n    def __init__(self, features_file, labels_file=None):\n        self.features = np.load(features_file)  # Shape: (num_samples, num_features)\n        self.labels = np.load(labels_file) if labels_file else None\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n        \n        if self.labels is not None:\n            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n            return feature, label\n        return feature\n\n# Simple fully connected model for deepfake detection\nclass VideoDeepfakeModel(nn.Module):\n    def __init__(self, input_size=1000, hidden_size=512, dropout_prob=0.2):\n        super(VideoDeepfakeModel, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.Dropout(dropout_prob),\n            nn.Linear(hidden_size, 256),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.Dropout(dropout_prob),\n            nn.Linear(256, 1)  # Final layer for binary classification\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n    \ndef calculate_accuracy(outputs, labels, threshold=0.5):\n    # Apply sigmoid to the outputs to convert logits to probabilities\n    probabilities = torch.sigmoid(outputs)\n    \n    # Convert probabilities to binary predictions using the threshold\n    predictions = (probabilities >= threshold).float()\n    \n    # Compare the predictions with the actual labels\n    correct_predictions = (predictions == labels).float()\n    \n    # Calculate accuracy as the average of correct predictions\n    accuracy = correct_predictions.sum() / len(labels)\n    \n    return accuracy.item()\n\n# Compute Equal Error Rate (EER) for the given labels and predictions\ndef compute_eer(label, pred, positive_label=1):\n    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n    fnr = 1 - tpr\n    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer = (eer_1 + eer_2) / 2\n    return eer\n\n# Preprocessing for ResNet input\ndef preprocess_frame(frame):\n    preprocess = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    return preprocess(frame)\n\n# Use a pre-trained ResNet18 model for feature extraction\ndef extract_features_from_video_frames(frames_dir, device):\n    # Load pre-trained ResNet model\n    resnet = models.resnet18(pretrained=True)\n    resnet = resnet.to(device)\n    resnet.eval()  # Set the model to evaluation mode\n    \n    features_list = []\n    for frame_name in sorted(os.listdir(frames_dir)):\n        frame_path = os.path.join(frames_dir, frame_name)\n        \n        # Load and preprocess the image frame\n        frame = Image.open(frame_path).convert('RGB')\n        processed_frame = preprocess_frame(frame).unsqueeze(0).to(device)  # Add batch dimension and move to device\n\n        with torch.no_grad():\n                        features = torch.flatten(resnet(processed_frame)).cpu().numpy()\n\n        features_list.append(features)\n\n    # Convert the list of feature vectors to a numpy array\n    video_features = np.array(features_list)\n\n        if video_features.shape[1] != 1000:\n        pca = PCA(n_components=1000)\n        video_features = pca.fit_transform(video_features)\n\n    return video_features\n\n# Function to evaluate video for real/fake detection\ndef evaluate_video(frames_dir, model, device):\n    # Step 1: Extract features from the video frames\n    print(\"Extracting features from video frames...\")\n    video_features = extract_features_from_video_frames(frames_dir, device)\n    \n    if len(video_features) == 0:\n        print(\"No features extracted from the video. Cannot make a prediction.\")\n        return\n\n    # Step 2: Convert features to torch tensor and move to device\n    video_features_tensor = torch.tensor(video_features, dtype=torch.float32).to(device)\n\n    # Step 3: Pass the features through the model and get predictions\n    model.eval()  # Set model to evaluation mode\n    with torch.no_grad():\n        outputs = model(video_features_tensor).squeeze()\n\n    # Apply sigmoid to get probabilities\n    probabilities = torch.sigmoid(outputs).mean().item()\n\n        prediction = \"fake\" if probabilities >= 0.8 else \"real\"\n    \n    print(f\"Predicted: {prediction}, Probability: {probabilities:.4f}\")\n\n# Main training loop for model training and evaluation\ndef main(args):\n    # Load training dataset\n    train_dataset = NumpyVideoDeepfakeDataset(args['train_features_file'], args['train_labels_file'])\n    train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, num_workers=args['num_workers'])\n\n    # Initialize model, optimizer, and learning rate scheduler\n    model = VideoDeepfakeModel(input_size=args['input_size'], hidden_size=args['hidden_size'], dropout_prob=args['dropout_prob']).to(args['device'])\n    optimizer = torch.optim.AdamW(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n    criterion = nn.BCEWithLogitsLoss()\n    \n    lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\n    best_eer = float('inf')\n\n    for epoch in range(args['num_epochs']):\n        model.train()\n        total_loss = 0.0\n        total_accuracy = 0.0  # To track accuracy\n        all_labels = []\n        all_outputs = []\n\n        # Training loop\n        for features, labels in tqdm(train_loader, desc=f\"Epoch [{epoch + 1}/{args['num_epochs']}]\"):\n            features, labels = features.to(args['device']), labels.to(args['device'])\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(features).squeeze(dim=-1)  # Squeeze only the last dimension\n\n            # Ensure labels are also squeezed correctly for compatibility\n            labels = labels.float()  # Ensure labels are of float type for BCEWithLogitsLoss\n\n            # Compute loss\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n            # Calculate accuracy\n            batch_accuracy = calculate_accuracy(outputs, labels)\n            total_accuracy += batch_accuracy  # Accumulate accuracy over batches\n\n            all_labels.append(labels.cpu().numpy())\n            all_outputs.append(outputs.detach().cpu().numpy())\n\n        avg_loss = total_loss / len(train_loader)\n        avg_accuracy = total_accuracy / len(train_loader)  # Average accuracy over all batches\n        print(f\"Epoch [{epoch + 1}/{args['num_epochs']}], Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n\n        # Compute EER for training set\n        all_labels = np.concatenate(all_labels)\n        all_outputs = np.concatenate(all_outputs)\n        train_eer = compute_eer(all_labels, all_outputs)\n        print(f\"Epoch [{epoch + 1}/{args['num_epochs']}], Training EER: {train_eer:.4f}\")\n\n        # Save the model if EER improves\n        if train_eer < best_eer:\n            best_eer = train_eer\n            torch.save(model.state_dict(), 'best_model.pth')\n            print(\"Model saved as 'best_model.pth'\")\n\n        # Adjust learning rate based on average loss\n        lr_scheduler.step(avg_loss)\n\nif __name__ == \"__main__\":\n    args = {\n        'train_features_file': '/kaggle/input/embeddings-for-sih/small data feature extraction/features.npy',\n        'train_labels_file': '/kaggle/input/embeddings-for-sih/small data feature extraction/labels.npy',\n        'batch_size': 16,\n        'num_epochs': 25,\n        'lr': 1e-5,\n        'num_workers': 4,\n        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n        'input_size': 1000,  # Input size for the video model\n        'hidden_size': 512,\n        'dropout_prob': 0.2,\n        'weight_decay': 1e-5\n    }\n\n    main(args)\n\n    # Load the best model and evaluate a video\n    model = VideoDeepfakeModel(input_size=args['input_size'], hidden_size=args['hidden_size'], dropout_prob=args['dropout_prob']).to(args['device'])\n    model.load_state_dict(torch.load('/kaggle/working/best_model.pth', map_location=args['device'], weights_only=True))\n    evaluate_video('/kaggle/input/test-sih/test', model, args['device'])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T13:29:15.911143Z","iopub.execute_input":"2024-09-18T13:29:15.911428Z","iopub.status.idle":"2024-09-18T13:29:47.291444Z","shell.execute_reply.started":"2024-09-18T13:29:15.911396Z","shell.execute_reply":"2024-09-18T13:29:47.290366Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\nEpoch [1/25]: 100%|██████████| 235/235 [00:01<00:00, 171.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/25], Loss: 0.5156, Accuracy: 0.7886\nEpoch [1/25], Training EER: 0.4771\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/25]: 100%|██████████| 235/235 [00:00<00:00, 246.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/25], Loss: 0.4802, Accuracy: 0.7984\nEpoch [2/25], Training EER: 0.3816\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/25]: 100%|██████████| 235/235 [00:01<00:00, 215.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/25], Loss: 0.4679, Accuracy: 0.7939\nEpoch [3/25], Training EER: 0.3406\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/25]: 100%|██████████| 235/235 [00:00<00:00, 247.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/25], Loss: 0.4465, Accuracy: 0.8024\nEpoch [4/25], Training EER: 0.3182\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/25]: 100%|██████████| 235/235 [00:00<00:00, 240.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/25], Loss: 0.4313, Accuracy: 0.8048\nEpoch [5/25], Training EER: 0.2996\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/25]: 100%|██████████| 235/235 [00:00<00:00, 257.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/25], Loss: 0.4198, Accuracy: 0.8112\nEpoch [6/25], Training EER: 0.2911\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/25]: 100%|██████████| 235/235 [00:00<00:00, 262.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/25], Loss: 0.4112, Accuracy: 0.8186\nEpoch [7/25], Training EER: 0.2806\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/25]: 100%|██████████| 235/235 [00:00<00:00, 248.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/25], Loss: 0.3970, Accuracy: 0.8258\nEpoch [8/25], Training EER: 0.2581\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/25]: 100%|██████████| 235/235 [00:01<00:00, 219.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/25], Loss: 0.3906, Accuracy: 0.8269\nEpoch [9/25], Training EER: 0.2541\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/25]: 100%|██████████| 235/235 [00:00<00:00, 241.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/25], Loss: 0.3794, Accuracy: 0.8343\nEpoch [10/25], Training EER: 0.2356\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [11/25]: 100%|██████████| 235/235 [00:00<00:00, 256.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/25], Loss: 0.3673, Accuracy: 0.8431\nEpoch [11/25], Training EER: 0.2318\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [12/25]: 100%|██████████| 235/235 [00:00<00:00, 255.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/25], Loss: 0.3554, Accuracy: 0.8487\nEpoch [12/25], Training EER: 0.2164\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [13/25]: 100%|██████████| 235/235 [00:00<00:00, 256.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/25], Loss: 0.3471, Accuracy: 0.8535\nEpoch [13/25], Training EER: 0.2217\n","output_type":"stream"},{"name":"stderr","text":"Epoch [14/25]: 100%|██████████| 235/235 [00:00<00:00, 242.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/25], Loss: 0.3522, Accuracy: 0.8532\nEpoch [14/25], Training EER: 0.2067\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [15/25]: 100%|██████████| 235/235 [00:00<00:00, 244.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/25], Loss: 0.3320, Accuracy: 0.8598\nEpoch [15/25], Training EER: 0.2001\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [16/25]: 100%|██████████| 235/235 [00:00<00:00, 260.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/25], Loss: 0.3220, Accuracy: 0.8670\nEpoch [16/25], Training EER: 0.1818\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [17/25]: 100%|██████████| 235/235 [00:00<00:00, 266.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/25], Loss: 0.3098, Accuracy: 0.8729\nEpoch [17/25], Training EER: 0.1724\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [18/25]: 100%|██████████| 235/235 [00:00<00:00, 264.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [18/25], Loss: 0.3084, Accuracy: 0.8769\nEpoch [18/25], Training EER: 0.1715\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [19/25]: 100%|██████████| 235/235 [00:00<00:00, 256.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [19/25], Loss: 0.2995, Accuracy: 0.8790\nEpoch [19/25], Training EER: 0.1739\n","output_type":"stream"},{"name":"stderr","text":"Epoch [20/25]: 100%|██████████| 235/235 [00:00<00:00, 260.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [20/25], Loss: 0.2918, Accuracy: 0.8787\nEpoch [20/25], Training EER: 0.1634\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [21/25]: 100%|██████████| 235/235 [00:00<00:00, 259.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [21/25], Loss: 0.2872, Accuracy: 0.8803\nEpoch [21/25], Training EER: 0.1556\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [22/25]: 100%|██████████| 235/235 [00:00<00:00, 259.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [22/25], Loss: 0.2802, Accuracy: 0.8896\nEpoch [22/25], Training EER: 0.1566\n","output_type":"stream"},{"name":"stderr","text":"Epoch [23/25]: 100%|██████████| 235/235 [00:00<00:00, 249.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [23/25], Loss: 0.2745, Accuracy: 0.8939\nEpoch [23/25], Training EER: 0.1446\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [24/25]: 100%|██████████| 235/235 [00:00<00:00, 254.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [24/25], Loss: 0.2645, Accuracy: 0.8939\nEpoch [24/25], Training EER: 0.1431\nModel saved as 'best_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"Epoch [25/25]: 100%|██████████| 235/235 [00:00<00:00, 255.62it/s]\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch [25/25], Loss: 0.2626, Accuracy: 0.8960\nEpoch [25/25], Training EER: 0.1431\nExtracting features from video frames...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 207MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Predicted: fake, Probability: 0.9476\n","output_type":"stream"}],"execution_count":1}]}
